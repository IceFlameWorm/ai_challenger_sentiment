{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(4444)\n",
    "import pickle\n",
    "import numpy as np\n",
    "np.random.seed(5555)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(6666)\n",
    "import pandas as pd\n",
    "from utils.dataset import DataSet, LABELS\n",
    "from models.own import OwnSingleModel\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from env import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_SEQS_PATH = os.path.join(CACHES_PATH, 'word_seqs/simple')\n",
    "\n",
    "TRAIN_SEQS_PADDED_PKL = os.path.join(WORD_SEQS_PATH, 'train_seqs_padded.pkl')\n",
    "VAL_SEQS_PADDED_PKL = os.path.join(WORD_SEQS_PATH, 'val_seqs_padded.pkl')\n",
    "TEST_SEQS_PADDED_PKL = os.path.join(WORD_SEQS_PATH, 'test_seqs_padded.pkl')\n",
    "\n",
    "MODEL_FILE = os.path.join(SAVED_MODELS_PATH, 'own/single.h5')\n",
    "RESULT_CSV = os.path.join(RESULTS_PATH, 'own_single.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = DataSet()\n",
    "train, val, test = raw_dataset.train, raw_dataset.val, raw_dataset.test\n",
    "\n",
    "with open(TRAIN_SEQS_PADDED_PKL, 'rb') as f:\n",
    "    train_seqs_padded = pickle.load(f)\n",
    "    \n",
    "with open(VAL_SEQS_PADDED_PKL, 'rb') as f:\n",
    "    val_seqs_padded = pickle.load(f)\n",
    "    \n",
    "with open(TEST_SEQS_PADDED_PKL, 'rb') as f:\n",
    "    test_seqs_padded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_seq = pd.merge(train, train_seqs_padded, on='id')\n",
    "val_with_seq = pd.merge(val, val_seqs_padded, on='id')\n",
    "test_with_seq = pd.merge(test, test_seqs_padded, on='id')\n",
    "\n",
    "seq = 'word_seq'\n",
    "train_x = np.array(list(train_with_seq[seq]))\n",
    "val_x = np.array(list(val_with_seq[seq]))\n",
    "test_x = np.array(list(test_with_seq[seq]))\n",
    "\n",
    "y_cols = LABELS\n",
    "val_ys = []\n",
    "train_outputs = []\n",
    "val_outputs = []\n",
    "for col in y_cols:\n",
    "    train_y = train_with_seq[col] + 2\n",
    "    val_y = val_with_seq[col] + 2\n",
    "    val_ys.append(val_y)\n",
    "    train_y_onehot = to_categorical(train_y)\n",
    "    val_y_onehot = to_categorical(val_y)\n",
    "    train_outputs.append(train_y_onehot)\n",
    "    val_outputs.append(val_y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OwnSingleModel(max_len = train_x.shape[1])\n",
    "# model._model.summary()\n",
    "plot_model(model._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/300\n",
      "105000/105000 [==============================] - 35s 335us/step - loss: 13.6784 - dense_1_loss: 0.3655 - dense_2_loss: 0.4615 - dense_3_loss: 0.5215 - dense_4_loss: 0.4988 - dense_5_loss: 0.8156 - dense_6_loss: 0.2465 - dense_7_loss: 0.4557 - dense_8_loss: 1.0615 - dense_9_loss: 0.6446 - dense_10_loss: 0.8465 - dense_11_loss: 0.7142 - dense_12_loss: 0.6451 - dense_13_loss: 0.8388 - dense_14_loss: 0.7336 - dense_15_loss: 1.0293 - dense_16_loss: 0.8897 - dense_17_loss: 0.7878 - dense_18_loss: 0.5572 - dense_19_loss: 0.7765 - dense_20_loss: 0.7883 - val_loss: 11.1526 - val_dense_1_loss: 0.2623 - val_dense_2_loss: 0.3539 - val_dense_3_loss: 0.4021 - val_dense_4_loss: 0.4723 - val_dense_5_loss: 0.6143 - val_dense_6_loss: 0.1465 - val_dense_7_loss: 0.3627 - val_dense_8_loss: 0.8644 - val_dense_9_loss: 0.4842 - val_dense_10_loss: 0.5884 - val_dense_11_loss: 0.5802 - val_dense_12_loss: 0.5674 - val_dense_13_loss: 0.7503 - val_dense_14_loss: 0.5951 - val_dense_15_loss: 0.8591 - val_dense_16_loss: 0.7855 - val_dense_17_loss: 0.7584 - val_dense_18_loss: 0.4418 - val_dense_19_loss: 0.6606 - val_dense_20_loss: 0.6030\n",
      "Epoch 2/300\n",
      "105000/105000 [==============================] - 33s 313us/step - loss: 10.3243 - dense_1_loss: 0.2393 - dense_2_loss: 0.3250 - dense_3_loss: 0.3783 - dense_4_loss: 0.4156 - dense_5_loss: 0.5718 - dense_6_loss: 0.1237 - dense_7_loss: 0.3361 - dense_8_loss: 0.7811 - dense_9_loss: 0.4266 - dense_10_loss: 0.5423 - dense_11_loss: 0.5338 - dense_12_loss: 0.5249 - dense_13_loss: 0.7260 - dense_14_loss: 0.5439 - dense_15_loss: 0.8120 - dense_16_loss: 0.7443 - dense_17_loss: 0.6992 - dense_18_loss: 0.4023 - dense_19_loss: 0.6260 - dense_20_loss: 0.5722 - val_loss: 10.1890 - val_dense_1_loss: 0.2319 - val_dense_2_loss: 0.3334 - val_dense_3_loss: 0.3726 - val_dense_4_loss: 0.3788 - val_dense_5_loss: 0.5798 - val_dense_6_loss: 0.1256 - val_dense_7_loss: 0.3383 - val_dense_8_loss: 0.7643 - val_dense_9_loss: 0.4037 - val_dense_10_loss: 0.5283 - val_dense_11_loss: 0.5433 - val_dense_12_loss: 0.5436 - val_dense_13_loss: 0.7221 - val_dense_14_loss: 0.5289 - val_dense_15_loss: 0.7859 - val_dense_16_loss: 0.7496 - val_dense_17_loss: 0.6784 - val_dense_18_loss: 0.4005 - val_dense_19_loss: 0.6299 - val_dense_20_loss: 0.5502\n",
      "Epoch 3/300\n",
      "105000/105000 [==============================] - 33s 312us/step - loss: 9.2502 - dense_1_loss: 0.2090 - dense_2_loss: 0.2916 - dense_3_loss: 0.3369 - dense_4_loss: 0.3551 - dense_5_loss: 0.5119 - dense_6_loss: 0.1107 - dense_7_loss: 0.2974 - dense_8_loss: 0.6855 - dense_9_loss: 0.3812 - dense_10_loss: 0.4977 - dense_11_loss: 0.4840 - dense_12_loss: 0.4757 - dense_13_loss: 0.6669 - dense_14_loss: 0.4743 - dense_15_loss: 0.7314 - dense_16_loss: 0.6820 - dense_17_loss: 0.6259 - dense_18_loss: 0.3655 - dense_19_loss: 0.5650 - dense_20_loss: 0.5026 - val_loss: 10.0150 - val_dense_1_loss: 0.2374 - val_dense_2_loss: 0.3418 - val_dense_3_loss: 0.3725 - val_dense_4_loss: 0.3539 - val_dense_5_loss: 0.5779 - val_dense_6_loss: 0.1253 - val_dense_7_loss: 0.3219 - val_dense_8_loss: 0.7342 - val_dense_9_loss: 0.4000 - val_dense_10_loss: 0.5236 - val_dense_11_loss: 0.5420 - val_dense_12_loss: 0.5297 - val_dense_13_loss: 0.7021 - val_dense_14_loss: 0.5168 - val_dense_15_loss: 0.7696 - val_dense_16_loss: 0.7531 - val_dense_17_loss: 0.6387 - val_dense_18_loss: 0.4130 - val_dense_19_loss: 0.6271 - val_dense_20_loss: 0.5345\n",
      "Epoch 4/300\n",
      "105000/105000 [==============================] - 33s 313us/step - loss: 8.4726 - dense_1_loss: 0.1894 - dense_2_loss: 0.2634 - dense_3_loss: 0.3039 - dense_4_loss: 0.3301 - dense_5_loss: 0.4526 - dense_6_loss: 0.1043 - dense_7_loss: 0.2683 - dense_8_loss: 0.6273 - dense_9_loss: 0.3609 - dense_10_loss: 0.4667 - dense_11_loss: 0.4428 - dense_12_loss: 0.4330 - dense_13_loss: 0.6174 - dense_14_loss: 0.4292 - dense_15_loss: 0.6694 - dense_16_loss: 0.6292 - dense_17_loss: 0.5822 - dense_18_loss: 0.3374 - dense_19_loss: 0.5166 - dense_20_loss: 0.4483 - val_loss: 9.9590 - val_dense_1_loss: 0.2444 - val_dense_2_loss: 0.3543 - val_dense_3_loss: 0.3764 - val_dense_4_loss: 0.3500 - val_dense_5_loss: 0.5378 - val_dense_6_loss: 0.1254 - val_dense_7_loss: 0.3162 - val_dense_8_loss: 0.7305 - val_dense_9_loss: 0.4041 - val_dense_10_loss: 0.5267 - val_dense_11_loss: 0.5496 - val_dense_12_loss: 0.5278 - val_dense_13_loss: 0.6933 - val_dense_14_loss: 0.5234 - val_dense_15_loss: 0.7527 - val_dense_16_loss: 0.7370 - val_dense_17_loss: 0.6343 - val_dense_18_loss: 0.4034 - val_dense_19_loss: 0.6212 - val_dense_20_loss: 0.5506\n",
      "Epoch 5/300\n",
      "105000/105000 [==============================] - 33s 314us/step - loss: 7.8075 - dense_1_loss: 0.1736 - dense_2_loss: 0.2376 - dense_3_loss: 0.2734 - dense_4_loss: 0.3123 - dense_5_loss: 0.4000 - dense_6_loss: 0.0992 - dense_7_loss: 0.2439 - dense_8_loss: 0.5791 - dense_9_loss: 0.3418 - dense_10_loss: 0.4412 - dense_11_loss: 0.4078 - dense_12_loss: 0.3924 - dense_13_loss: 0.5744 - dense_14_loss: 0.3921 - dense_15_loss: 0.6134 - dense_16_loss: 0.5820 - dense_17_loss: 0.5484 - dense_18_loss: 0.3120 - dense_19_loss: 0.4767 - dense_20_loss: 0.4064 - val_loss: 10.2276 - val_dense_1_loss: 0.2560 - val_dense_2_loss: 0.3717 - val_dense_3_loss: 0.3909 - val_dense_4_loss: 0.3537 - val_dense_5_loss: 0.5551 - val_dense_6_loss: 0.1261 - val_dense_7_loss: 0.3249 - val_dense_8_loss: 0.7460 - val_dense_9_loss: 0.4063 - val_dense_10_loss: 0.5413 - val_dense_11_loss: 0.5670 - val_dense_12_loss: 0.5497 - val_dense_13_loss: 0.7046 - val_dense_14_loss: 0.5387 - val_dense_15_loss: 0.7858 - val_dense_16_loss: 0.7450 - val_dense_17_loss: 0.6451 - val_dense_18_loss: 0.4186 - val_dense_19_loss: 0.6374 - val_dense_20_loss: 0.5637\n",
      "Epoch 6/300\n",
      "105000/105000 [==============================] - 33s 313us/step - loss: 7.2501 - dense_1_loss: 0.1614 - dense_2_loss: 0.2152 - dense_3_loss: 0.2488 - dense_4_loss: 0.2966 - dense_5_loss: 0.3662 - dense_6_loss: 0.0952 - dense_7_loss: 0.2256 - dense_8_loss: 0.5387 - dense_9_loss: 0.3248 - dense_10_loss: 0.4169 - dense_11_loss: 0.3784 - dense_12_loss: 0.3572 - dense_13_loss: 0.5378 - dense_14_loss: 0.3569 - dense_15_loss: 0.5653 - dense_16_loss: 0.5404 - dense_17_loss: 0.5170 - dense_18_loss: 0.2912 - dense_19_loss: 0.4441 - dense_20_loss: 0.3722 - val_loss: 10.5365 - val_dense_1_loss: 0.2674 - val_dense_2_loss: 0.4039 - val_dense_3_loss: 0.4077 - val_dense_4_loss: 0.3602 - val_dense_5_loss: 0.5637 - val_dense_6_loss: 0.1273 - val_dense_7_loss: 0.3289 - val_dense_8_loss: 0.7673 - val_dense_9_loss: 0.4173 - val_dense_10_loss: 0.5559 - val_dense_11_loss: 0.5883 - val_dense_12_loss: 0.5620 - val_dense_13_loss: 0.7261 - val_dense_14_loss: 0.5602 - val_dense_15_loss: 0.7897 - val_dense_16_loss: 0.7653 - val_dense_17_loss: 0.6633 - val_dense_18_loss: 0.4411 - val_dense_19_loss: 0.6618 - val_dense_20_loss: 0.5793\n",
      "Epoch 7/300\n",
      "105000/105000 [==============================] - 33s 313us/step - loss: 6.7657 - dense_1_loss: 0.1498 - dense_2_loss: 0.1961 - dense_3_loss: 0.2281 - dense_4_loss: 0.2823 - dense_5_loss: 0.3394 - dense_6_loss: 0.0925 - dense_7_loss: 0.2103 - dense_8_loss: 0.5038 - dense_9_loss: 0.3094 - dense_10_loss: 0.3976 - dense_11_loss: 0.3535 - dense_12_loss: 0.3289 - dense_13_loss: 0.5036 - dense_14_loss: 0.3278 - dense_15_loss: 0.5228 - dense_16_loss: 0.5034 - dense_17_loss: 0.4873 - dense_18_loss: 0.2708 - dense_19_loss: 0.4181 - dense_20_loss: 0.3404 - val_loss: 10.9758 - val_dense_1_loss: 0.2821 - val_dense_2_loss: 0.4282 - val_dense_3_loss: 0.4287 - val_dense_4_loss: 0.3687 - val_dense_5_loss: 0.5901 - val_dense_6_loss: 0.1348 - val_dense_7_loss: 0.3398 - val_dense_8_loss: 0.7920 - val_dense_9_loss: 0.4298 - val_dense_10_loss: 0.5719 - val_dense_11_loss: 0.6190 - val_dense_12_loss: 0.5906 - val_dense_13_loss: 0.7462 - val_dense_14_loss: 0.5918 - val_dense_15_loss: 0.8232 - val_dense_16_loss: 0.7919 - val_dense_17_loss: 0.6802 - val_dense_18_loss: 0.4581 - val_dense_19_loss: 0.6942 - val_dense_20_loss: 0.6146\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105000/105000 [==============================] - 33s 313us/step - loss: 5.8740 - dense_1_loss: 0.1288 - dense_2_loss: 0.1587 - dense_3_loss: 0.1920 - dense_4_loss: 0.2545 - dense_5_loss: 0.2934 - dense_6_loss: 0.0844 - dense_7_loss: 0.1816 - dense_8_loss: 0.4373 - dense_9_loss: 0.2783 - dense_10_loss: 0.3567 - dense_11_loss: 0.3059 - dense_12_loss: 0.2792 - dense_13_loss: 0.4466 - dense_14_loss: 0.2763 - dense_15_loss: 0.4444 - dense_16_loss: 0.4408 - dense_17_loss: 0.4305 - dense_18_loss: 0.2320 - dense_19_loss: 0.3687 - dense_20_loss: 0.2839 - val_loss: 11.2011 - val_dense_1_loss: 0.2916 - val_dense_2_loss: 0.4395 - val_dense_3_loss: 0.4421 - val_dense_4_loss: 0.3735 - val_dense_5_loss: 0.6034 - val_dense_6_loss: 0.1357 - val_dense_7_loss: 0.3464 - val_dense_8_loss: 0.8096 - val_dense_9_loss: 0.4339 - val_dense_10_loss: 0.5683 - val_dense_11_loss: 0.6241 - val_dense_12_loss: 0.5974 - val_dense_13_loss: 0.7608 - val_dense_14_loss: 0.6094 - val_dense_15_loss: 0.8372 - val_dense_16_loss: 0.8121 - val_dense_17_loss: 0.6960 - val_dense_18_loss: 0.4689 - val_dense_19_loss: 0.7139 - val_dense_20_loss: 0.6375\n",
      "Epoch 9/300\n",
      "105000/105000 [==============================] - 33s 313us/step - loss: 5.5991 - dense_1_loss: 0.1223 - dense_2_loss: 0.1480 - dense_3_loss: 0.1806 - dense_4_loss: 0.2472 - dense_5_loss: 0.2802 - dense_6_loss: 0.0824 - dense_7_loss: 0.1735 - dense_8_loss: 0.4177 - dense_9_loss: 0.2693 - dense_10_loss: 0.3449 - dense_11_loss: 0.2912 - dense_12_loss: 0.2632 - dense_13_loss: 0.4266 - dense_14_loss: 0.2615 - dense_15_loss: 0.4211 - dense_16_loss: 0.4189 - dense_17_loss: 0.4135 - dense_18_loss: 0.2215 - dense_19_loss: 0.3494 - dense_20_loss: 0.2659 - val_loss: 11.5251 - val_dense_1_loss: 0.3000 - val_dense_2_loss: 0.4590 - val_dense_3_loss: 0.4565 - val_dense_4_loss: 0.3812 - val_dense_5_loss: 0.6195 - val_dense_6_loss: 0.1368 - val_dense_7_loss: 0.3547 - val_dense_8_loss: 0.8290 - val_dense_9_loss: 0.4404 - val_dense_10_loss: 0.5806 - val_dense_11_loss: 0.6420 - val_dense_12_loss: 0.6202 - val_dense_13_loss: 0.7814 - val_dense_14_loss: 0.6274 - val_dense_15_loss: 0.8622 - val_dense_16_loss: 0.8339 - val_dense_17_loss: 0.7148 - val_dense_18_loss: 0.4859 - val_dense_19_loss: 0.7361 - val_dense_20_loss: 0.6636\n",
      "Epoch 10/300\n",
      "105000/105000 [==============================] - 33s 313us/step - loss: 5.4105 - dense_1_loss: 0.1185 - dense_2_loss: 0.1409 - dense_3_loss: 0.1734 - dense_4_loss: 0.2418 - dense_5_loss: 0.2710 - dense_6_loss: 0.0810 - dense_7_loss: 0.1685 - dense_8_loss: 0.4036 - dense_9_loss: 0.2629 - dense_10_loss: 0.3368 - dense_11_loss: 0.2819 - dense_12_loss: 0.2528 - dense_13_loss: 0.4131 - dense_14_loss: 0.2507 - dense_15_loss: 0.4047 - dense_16_loss: 0.4035 - dense_17_loss: 0.4007 - dense_18_loss: 0.2151 - dense_19_loss: 0.3362 - dense_20_loss: 0.2535 - val_loss: 11.7974 - val_dense_1_loss: 0.3082 - val_dense_2_loss: 0.4752 - val_dense_3_loss: 0.4690 - val_dense_4_loss: 0.3867 - val_dense_5_loss: 0.6335 - val_dense_6_loss: 0.1400 - val_dense_7_loss: 0.3603 - val_dense_8_loss: 0.8456 - val_dense_9_loss: 0.4492 - val_dense_10_loss: 0.5897 - val_dense_11_loss: 0.6592 - val_dense_12_loss: 0.6348 - val_dense_13_loss: 0.7957 - val_dense_14_loss: 0.6441 - val_dense_15_loss: 0.8844 - val_dense_16_loss: 0.8531 - val_dense_17_loss: 0.7290 - val_dense_18_loss: 0.4971 - val_dense_19_loss: 0.7582 - val_dense_20_loss: 0.6844\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_outputs, \n",
    "          validation_data = (val_x, val_outputs),\n",
    "          model_file = MODEL_FILE\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.load_weights(MODEL_FILE)\n",
    "val_probs = model.predict(val_x)\n",
    "test_probs = model.predict(test_x)\n",
    "\n",
    "val_preds = list(map(lambda x: np.argmax(x, axis = -1), val_probs))\n",
    "# val_preds = np.argmax(val_probs, axis = 1)\n",
    "# test_preds = list(map(lambda x: np.argmax(x, axis = -1), test_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th f1: 0.4596354033490769\n",
      "The 1th f1: 0.3929970327642951\n",
      "The 2th f1: 0.5295040983037937\n",
      "The 3th f1: 0.4305639940330617\n",
      "The 4th f1: 0.6912658532749753\n",
      "The 5th f1: 0.4937591469268501\n",
      "The 6th f1: 0.5591111037496518\n",
      "The 7th f1: 0.6408409780561068\n",
      "The 8th f1: 0.4915788789772023\n",
      "The 9th f1: 0.5435962254233646\n",
      "The 10th f1: 0.5277953515975469\n",
      "The 11th f1: 0.5414153841463157\n",
      "The 12th f1: 0.49896178686251547\n",
      "The 13th f1: 0.550762413602188\n",
      "The 14th f1: 0.5663567536545737\n",
      "The 15th f1: 0.5218138169250428\n",
      "The 16th f1: 0.40697435509378554\n",
      "The 17th f1: 0.48924305539901836\n",
      "The 18th f1: 0.48875205269084987\n",
      "The 19th f1: 0.5445940121789128\n",
      "The average f1 of val is 0.5184760848504564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "f1s = 0\n",
    "for i, (vy, vp) in enumerate(zip(val_ys, val_preds)):\n",
    "    f1 = f1_score(vy, vp, average='macro')\n",
    "    print(\"The %sth f1: %s\" % (i, f1))\n",
    "    f1s += f1\n",
    "    \n",
    "print(\"The average f1 of val is %s\" % (f1s / len(y_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
